{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames have been saved to D:\\Coding\\Files\\Project\\Construction 2.0\\data processing (image)\n",
      "Labels have been saved to D:\\Coding\\Files\\Project\\Construction 2.0\\data processing (image)\\labels.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def video_to_frames(video_path, output_folder, video_id, skip_seconds=20, interval_seconds=5):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "\n",
    "    frames_to_skip = int(skip_seconds * fps)\n",
    "    frames_per_interval = int(interval_seconds * fps)\n",
    "\n",
    "    vidcap.set(cv2.CAP_PROP_POS_FRAMES, frames_to_skip)\n",
    "\n",
    "    frames = []\n",
    "    frame_number = frames_to_skip\n",
    "\n",
    "    while True:\n",
    "        success, image = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if (frame_number - frames_to_skip) % frames_per_interval == 0:\n",
    "            frame_filename = f\"video_{video_id}_frame_{frame_number}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "            cv2.imwrite(frame_path, image)\n",
    "\n",
    "            frames.append({\n",
    "                'frame_number': frame_number,\n",
    "                'filename': frame_filename\n",
    "            })\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    vidcap.release()\n",
    "    return frames\n",
    "\n",
    "def create_labels(num_frames):\n",
    "    return np.linspace(0, 1, num_frames)\n",
    "\n",
    "def process_videos(video_folder, output_folder):\n",
    "    video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4') or f.endswith('.avi')]\n",
    "    video_files.sort()  # Ensure consistent ordering\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i, video_file in enumerate(video_files):\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        frames = video_to_frames(video_path, output_folder, i + 1)\n",
    "        video_labels = create_labels(len(frames))\n",
    "        \n",
    "        for j, (frame, label) in enumerate(zip(frames, video_labels)):\n",
    "            labels.append({\n",
    "                'video_id': i + 1,\n",
    "                'frame_number': frame['frame_number'],\n",
    "                'filename': frame['filename'],\n",
    "                'percentage': label\n",
    "            })\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def save_labels_to_csv(labels, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['video_id', 'frame_number', 'filename', 'percentage']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for label in labels:\n",
    "            writer.writerow(label)\n",
    "\n",
    "# Main execution\n",
    "video_folder = r'D:\\Coding\\Files\\Project\\Construction 2.0\\Ml AI'\n",
    "output_folder = r'D:\\Coding\\Files\\Project\\Construction 2.0\\data processing (image)'\n",
    "output_file = os.path.join(output_folder, 'labels.csv')\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "labels = process_videos(video_folder, output_folder)\n",
    "save_labels_to_csv(labels, output_file)\n",
    "\n",
    "print(f\"Frames have been saved to {output_folder}\")\n",
    "print(f\"Labels have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 908ms/step - loss: 2.1338 - mae: 1.1481 - val_loss: 0.4721 - val_mae: 0.5691 - learning_rate: 0.0010\n",
      "Epoch 2/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 715ms/step - loss: 1.6337 - mae: 1.0095 - val_loss: 0.2294 - val_mae: 0.3889 - learning_rate: 0.0010\n",
      "Epoch 3/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 675ms/step - loss: 0.9951 - mae: 0.7767 - val_loss: 0.3610 - val_mae: 0.4977 - learning_rate: 0.0010\n",
      "Epoch 4/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 690ms/step - loss: 0.9057 - mae: 0.7631 - val_loss: 0.3012 - val_mae: 0.4517 - learning_rate: 0.0010\n",
      "Epoch 5/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 707ms/step - loss: 0.7448 - mae: 0.6914 - val_loss: 0.1797 - val_mae: 0.3523 - learning_rate: 0.0010\n",
      "Epoch 6/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 694ms/step - loss: 0.6919 - mae: 0.6598 - val_loss: 0.1242 - val_mae: 0.2829 - learning_rate: 0.0010\n",
      "Epoch 7/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 703ms/step - loss: 0.6243 - mae: 0.6333 - val_loss: 0.0944 - val_mae: 0.2424 - learning_rate: 0.0010\n",
      "Epoch 8/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 699ms/step - loss: 0.5102 - mae: 0.5617 - val_loss: 0.0896 - val_mae: 0.2442 - learning_rate: 0.0010\n",
      "Epoch 9/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 699ms/step - loss: 0.4184 - mae: 0.5222 - val_loss: 0.0786 - val_mae: 0.2245 - learning_rate: 0.0010\n",
      "Epoch 10/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 711ms/step - loss: 0.4184 - mae: 0.4930 - val_loss: 0.0832 - val_mae: 0.2398 - learning_rate: 0.0010\n",
      "Epoch 11/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 697ms/step - loss: 0.4266 - mae: 0.5171 - val_loss: 0.1054 - val_mae: 0.2737 - learning_rate: 0.0010\n",
      "Epoch 12/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 689ms/step - loss: 0.4161 - mae: 0.5024 - val_loss: 0.0897 - val_mae: 0.2438 - learning_rate: 0.0010\n",
      "Epoch 13/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.3559 - mae: 0.4825 - val_loss: 0.0789 - val_mae: 0.2203 - learning_rate: 0.0010\n",
      "Epoch 14/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 706ms/step - loss: 0.3457 - mae: 0.4425 - val_loss: 0.0621 - val_mae: 0.1863 - learning_rate: 0.0010\n",
      "Epoch 15/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 682ms/step - loss: 0.3280 - mae: 0.4414 - val_loss: 0.0634 - val_mae: 0.1939 - learning_rate: 0.0010\n",
      "Epoch 16/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 694ms/step - loss: 0.2712 - mae: 0.4051 - val_loss: 0.0565 - val_mae: 0.1873 - learning_rate: 0.0010\n",
      "Epoch 17/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 698ms/step - loss: 0.2398 - mae: 0.3763 - val_loss: 0.0566 - val_mae: 0.1909 - learning_rate: 0.0010\n",
      "Epoch 18/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 731ms/step - loss: 0.2821 - mae: 0.4326 - val_loss: 0.0609 - val_mae: 0.2051 - learning_rate: 0.0010\n",
      "Epoch 19/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 681ms/step - loss: 0.2434 - mae: 0.3774 - val_loss: 0.0524 - val_mae: 0.1819 - learning_rate: 0.0010\n",
      "Epoch 20/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 671ms/step - loss: 0.2154 - mae: 0.3653 - val_loss: 0.0560 - val_mae: 0.1822 - learning_rate: 0.0010\n",
      "Epoch 21/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 680ms/step - loss: 0.2040 - mae: 0.3572 - val_loss: 0.0528 - val_mae: 0.1822 - learning_rate: 0.0010\n",
      "Epoch 22/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 673ms/step - loss: 0.1600 - mae: 0.3168 - val_loss: 0.0439 - val_mae: 0.1652 - learning_rate: 0.0010\n",
      "Epoch 23/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 681ms/step - loss: 0.1708 - mae: 0.3198 - val_loss: 0.0358 - val_mae: 0.1469 - learning_rate: 0.0010\n",
      "Epoch 24/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 674ms/step - loss: 0.1496 - mae: 0.3060 - val_loss: 0.0314 - val_mae: 0.1365 - learning_rate: 0.0010\n",
      "Epoch 25/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 678ms/step - loss: 0.1603 - mae: 0.3118 - val_loss: 0.0303 - val_mae: 0.1346 - learning_rate: 0.0010\n",
      "Epoch 26/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 680ms/step - loss: 0.1355 - mae: 0.2916 - val_loss: 0.0289 - val_mae: 0.1295 - learning_rate: 0.0010\n",
      "Epoch 27/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 726ms/step - loss: 0.1329 - mae: 0.2947 - val_loss: 0.0275 - val_mae: 0.1251 - learning_rate: 0.0010\n",
      "Epoch 28/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.1281 - mae: 0.2783 - val_loss: 0.0248 - val_mae: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 29/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 699ms/step - loss: 0.1158 - mae: 0.2709 - val_loss: 0.0245 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 30/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 687ms/step - loss: 0.1041 - mae: 0.2529 - val_loss: 0.0255 - val_mae: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 31/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.1031 - mae: 0.2477 - val_loss: 0.0236 - val_mae: 0.1165 - learning_rate: 0.0010\n",
      "Epoch 32/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 685ms/step - loss: 0.1003 - mae: 0.2588 - val_loss: 0.0221 - val_mae: 0.1092 - learning_rate: 0.0010\n",
      "Epoch 33/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 688ms/step - loss: 0.0947 - mae: 0.2425 - val_loss: 0.0217 - val_mae: 0.1067 - learning_rate: 0.0010\n",
      "Epoch 34/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 678ms/step - loss: 0.0894 - mae: 0.2299 - val_loss: 0.0236 - val_mae: 0.1134 - learning_rate: 0.0010\n",
      "Epoch 35/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 694ms/step - loss: 0.0952 - mae: 0.2482 - val_loss: 0.0240 - val_mae: 0.1147 - learning_rate: 0.0010\n",
      "Epoch 36/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 686ms/step - loss: 0.0808 - mae: 0.2273 - val_loss: 0.0235 - val_mae: 0.1132 - learning_rate: 0.0010\n",
      "Epoch 37/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 695ms/step - loss: 0.0825 - mae: 0.2250 - val_loss: 0.0221 - val_mae: 0.1106 - learning_rate: 0.0010\n",
      "Epoch 38/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 706ms/step - loss: 0.0681 - mae: 0.2086 - val_loss: 0.0214 - val_mae: 0.1080 - learning_rate: 0.0010\n",
      "Epoch 39/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.0612 - mae: 0.1911 - val_loss: 0.0208 - val_mae: 0.1062 - learning_rate: 0.0010\n",
      "Epoch 40/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 682ms/step - loss: 0.0561 - mae: 0.1881 - val_loss: 0.0213 - val_mae: 0.1072 - learning_rate: 0.0010\n",
      "Epoch 41/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 678ms/step - loss: 0.0649 - mae: 0.1996 - val_loss: 0.0200 - val_mae: 0.1043 - learning_rate: 0.0010\n",
      "Epoch 42/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 698ms/step - loss: 0.0585 - mae: 0.1901 - val_loss: 0.0191 - val_mae: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 43/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 688ms/step - loss: 0.0602 - mae: 0.1983 - val_loss: 0.0188 - val_mae: 0.1009 - learning_rate: 0.0010\n",
      "Epoch 44/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 692ms/step - loss: 0.0528 - mae: 0.1801 - val_loss: 0.0183 - val_mae: 0.0996 - learning_rate: 0.0010\n",
      "Epoch 45/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 700ms/step - loss: 0.0537 - mae: 0.1821 - val_loss: 0.0183 - val_mae: 0.0997 - learning_rate: 0.0010\n",
      "Epoch 46/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.0583 - mae: 0.1947 - val_loss: 0.0183 - val_mae: 0.0979 - learning_rate: 0.0010\n",
      "Epoch 47/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 693ms/step - loss: 0.0504 - mae: 0.1788 - val_loss: 0.0187 - val_mae: 0.0983 - learning_rate: 0.0010\n",
      "Epoch 48/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 707ms/step - loss: 0.0479 - mae: 0.1728 - val_loss: 0.0188 - val_mae: 0.0995 - learning_rate: 0.0010\n",
      "Epoch 49/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 701ms/step - loss: 0.0443 - mae: 0.1633 - val_loss: 0.0185 - val_mae: 0.0985 - learning_rate: 0.0010\n",
      "Epoch 50/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 693ms/step - loss: 0.0431 - mae: 0.1617 - val_loss: 0.0183 - val_mae: 0.0979 - learning_rate: 2.0000e-04\n",
      "Epoch 51/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 683ms/step - loss: 0.0449 - mae: 0.1691 - val_loss: 0.0181 - val_mae: 0.0970 - learning_rate: 2.0000e-04\n",
      "Epoch 52/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 688ms/step - loss: 0.0413 - mae: 0.1615 - val_loss: 0.0181 - val_mae: 0.0969 - learning_rate: 2.0000e-04\n",
      "Epoch 53/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 687ms/step - loss: 0.0398 - mae: 0.1579 - val_loss: 0.0181 - val_mae: 0.0971 - learning_rate: 2.0000e-04\n",
      "Epoch 54/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 705ms/step - loss: 0.0379 - mae: 0.1519 - val_loss: 0.0181 - val_mae: 0.0968 - learning_rate: 2.0000e-04\n",
      "Epoch 55/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 674ms/step - loss: 0.0442 - mae: 0.1660 - val_loss: 0.0179 - val_mae: 0.0965 - learning_rate: 2.0000e-04\n",
      "Epoch 56/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 703ms/step - loss: 0.0426 - mae: 0.1633 - val_loss: 0.0179 - val_mae: 0.0962 - learning_rate: 2.0000e-04\n",
      "Epoch 57/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 691ms/step - loss: 0.0486 - mae: 0.1711 - val_loss: 0.0178 - val_mae: 0.0961 - learning_rate: 2.0000e-04\n",
      "Epoch 58/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 690ms/step - loss: 0.0449 - mae: 0.1680 - val_loss: 0.0177 - val_mae: 0.0961 - learning_rate: 2.0000e-04\n",
      "Epoch 59/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 681ms/step - loss: 0.0464 - mae: 0.1648 - val_loss: 0.0177 - val_mae: 0.0963 - learning_rate: 2.0000e-04\n",
      "Epoch 60/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 693ms/step - loss: 0.0516 - mae: 0.1768 - val_loss: 0.0180 - val_mae: 0.0969 - learning_rate: 2.0000e-04\n",
      "Epoch 61/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 691ms/step - loss: 0.0419 - mae: 0.1654 - val_loss: 0.0179 - val_mae: 0.0969 - learning_rate: 2.0000e-04\n",
      "Epoch 62/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 692ms/step - loss: 0.0470 - mae: 0.1694 - val_loss: 0.0178 - val_mae: 0.0965 - learning_rate: 2.0000e-04\n",
      "Epoch 63/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 707ms/step - loss: 0.0396 - mae: 0.1538 - val_loss: 0.0178 - val_mae: 0.0965 - learning_rate: 2.0000e-04\n",
      "Epoch 64/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 696ms/step - loss: 0.0379 - mae: 0.1565 - val_loss: 0.0176 - val_mae: 0.0962 - learning_rate: 2.0000e-04\n",
      "Epoch 65/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 688ms/step - loss: 0.0423 - mae: 0.1603 - val_loss: 0.0176 - val_mae: 0.0960 - learning_rate: 4.0000e-05\n",
      "Epoch 66/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 696ms/step - loss: 0.0410 - mae: 0.1610 - val_loss: 0.0176 - val_mae: 0.0958 - learning_rate: 4.0000e-05\n",
      "Epoch 67/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 677ms/step - loss: 0.0483 - mae: 0.1769 - val_loss: 0.0177 - val_mae: 0.0963 - learning_rate: 4.0000e-05\n",
      "Epoch 68/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 693ms/step - loss: 0.0387 - mae: 0.1536 - val_loss: 0.0177 - val_mae: 0.0964 - learning_rate: 4.0000e-05\n",
      "Epoch 69/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 668ms/step - loss: 0.0491 - mae: 0.1736 - val_loss: 0.0178 - val_mae: 0.0968 - learning_rate: 4.0000e-05\n",
      "Epoch 70/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 695ms/step - loss: 0.0385 - mae: 0.1550 - val_loss: 0.0178 - val_mae: 0.0968 - learning_rate: 4.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Absolute Error: 0.0991\n",
      "Enhanced model has been trained and saved as 'building_completion_model_enhanced.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (224, 224)  # MobileNetV2 default input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 70  # Increased epochs, we'll use early stopping\n",
    "\n",
    "def load_dataset(csv_file, image_folder):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(image_folder, row['filename'])\n",
    "        img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)  # Preprocess for MobileNetV2\n",
    "        X.append(img_array)\n",
    "        y.append(row['percentage'])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_model():\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = create_model()\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=1)\n",
    "    return model, history\n",
    "\n",
    "def predict_completion(model, image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    return prediction\n",
    "\n",
    "# Main execution\n",
    "csv_file = r'D:\\Coding\\Files\\Project\\Construction 2.0\\data processing (image)\\labels.csv'\n",
    "image_folder = r'D:\\Coding\\Files\\Project\\Construction 2.0\\data processing (image)'\n",
    "\n",
    "# Load and prepare the dataset\n",
    "X, y = load_dataset(csv_file, image_folder)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Mean Absolute Error: {test_mae:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('building_completion_model_enhanced.h5')\n",
    "\n",
    "print(\"Enhanced model has been trained and saved as 'building_completion_model_enhanced.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history plot has been saved as 'training_history.png'\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Training history plot has been saved as 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predicted completion percentage: 43.07%\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the trained model\n",
    "example_image_path = r'D:\\Coding\\Files\\Project\\Construction 2.0\\WhatsApp Image 2024-09-18 at 03.23.29_2f759c41.jpg'  # Replace with an actual image path\n",
    "completion_percentage = predict_completion(model, example_image_path)\n",
    "print(f\"Predicted completion percentage: {completion_percentage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from D:\\Coding\\Files\\Project\\Construction 2.0\\building_completion_model_enhanced.h5\n",
      "Thank you for using the Building Completion Predictor!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (224, 224)  # Must match the size used during training\n",
    "MODEL_PATH = r'D:\\Coding\\Files\\Project\\Construction 2.0\\building_completion_model_enhanced.h5'  # Path to your saved model\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def predict_completion(model, image_path):\n",
    "    processed_image = load_and_preprocess_image(image_path)\n",
    "    prediction = model.predict(processed_image)[0][0]\n",
    "    return prediction\n",
    "\n",
    "def main():\n",
    "    # Load the saved model\n",
    "    model = load_model(MODEL_PATH)\n",
    "    print(f\"Model loaded from {MODEL_PATH}\")\n",
    "\n",
    "    while True:\n",
    "        # Get image path from user\n",
    "        image_path = input(\"\\nEnter the path to the building image (or 'q' to quit): \")\n",
    "        \n",
    "        if image_path.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"Error: File does not exist. Please enter a valid file path.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Make prediction\n",
    "            completion_percentage = predict_completion(model, image_path)\n",
    "            print(f\"Predicted completion percentage: {completion_percentage:.2%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    print(\"Thank you for using the Building Completion Predictor!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
